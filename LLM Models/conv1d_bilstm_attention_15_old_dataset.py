# -*- coding: utf-8 -*-
"""Conv1D_BiLSTM_ATTENTION_15_Old_Dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jbSn3nH9u2T3IKuXjfUevXDzPjTeQ4kK
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import re
from nltk.corpus import stopwords
from bs4 import BeautifulSoup

df = pd.read_csv('/content/deceptive-opinion.csv')

df.head()

df = df.drop(["hotel", "polarity","source"], axis=1)

df.head()

df1=df.sample(frac=1)

df1.head()

from sklearn import preprocessing

# label_encoder object knows how to understand word labels.
label_encoder = preprocessing.LabelEncoder()

# Encode labels in column 'species'.
df1['deceptive']= label_encoder.fit_transform(df1['deceptive'])

df1['deceptive'].unique()

print(list(label_encoder.inverse_transform([0,1])))

df1.head()

df1.describe()

X = df1.text
y = df1.deceptive

# Some preprocesssing that will be common to all the text classification methods

puncts = [',', '.', '"', ':', ')', '(', '-', '!', '?', '|', ';', "'", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\', '•',  '~', '@', '£',
 '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…',
 '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─',
 '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞',
 '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]

def clean_text(x):
    x = str(x)
    for punct in puncts:
        if punct in x:
            x = x.replace(punct, f' {punct} ')
    return x

def clean_numbers(x):
    if bool(re.search(r'\d', x)):
        x = re.sub('[0-9]{5,}', '#####', x)
        x = re.sub('[0-9]{4}', '####', x)
        x = re.sub('[0-9]{3}', '###', x)
        x = re.sub('[0-9]{2}', '##', x)
    return x

df1.head()

import nltk
nltk.download('stopwords')

REPLACE_BY_SPACE_RE = re.compile('[/(){}\[\]\|@,;]')
BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')
STOPWORDS = set(stopwords.words('english'))

def clean_data(text):
    """
        text: a string

        return: modified initial string
    """
    text = BeautifulSoup(text, "lxml").text # HTML decoding
    text = text.lower().split()
    text = " ".join(text)
    text = re.sub(r"[^A-Za-z0-9^,!.\/'+\-=]", " ", text)
    text = re.sub(r"what's", "what is ", text)
    text = re.sub(r"\'s", " ", text)
    text = re.sub(r"\'ve", " have ", text)
    text = re.sub(r"can't", "cannot ", text)
    text = re.sub(r"n't", " not ", text)
    text = re.sub(r"i'm", "i am ", text)
    text = re.sub(r"\'re", " are ", text)
    text = re.sub(r"\'d", " would ", text)
    text = re.sub(r"\'ll", " will ", text)
    text = re.sub(r",", " ", text)
    text = re.sub(r"\.", " ", text)
    text = re.sub(r"!", " ! ", text)
    text = re.sub(r"\/", " ", text)
    text = re.sub(r"\^", " ^ ", text)
    text = re.sub(r"\+", " + ", text)
    text = re.sub(r"\-", " - ", text)
    text = re.sub(r"\=", " = ", text)
    text = re.sub(r"'", " ", text)
    text = re.sub(r"(\d+)(k)", r"\g<1>000", text)
    text = re.sub(r":", " : ", text)
    text = re.sub(r" e g ", " eg ", text)
    text = re.sub(r" b g ", " bg ", text)
    text = re.sub(r" u s ", " american ", text)
    text = re.sub(r"\0s", "0", text)
    text = re.sub(r" 9 11 ", "911", text)
    text = re.sub(r"e - mail", "email", text)
    text = re.sub(r"j k", "jk", text)
    text = re.sub(r"\s{2,}", " ", text)

    return text

df1['text'] = df1['text'].apply(clean_data)

X = df1.text
y = df1.deceptive

X = X.map(lambda a: clean_data(a))

!pip install tensorflow

from __future__ import print_function
from tensorflow.keras.preprocessing.text import Tokenizer

import pandas as pd
import numpy as np
from bs4 import BeautifulSoup
# import logging
from numpy import random
# import gensim
import nltk
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.metrics import accuracy_score, confusion_matrix

from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, MaxPooling1D, Flatten
from keras.layers import Embedding
from keras.layers import Conv1D, GlobalMaxPooling1D, BatchNormalization
# from keras.callbacks import EarlyStopping
from keras.regularizers import l2
from keras import initializers, regularizers, constraints

train, test, y_train, y_test = train_test_split(X,y, test_size=0.2 ,stratify=y,random_state=42)

tokenizer = Tokenizer(num_words=None,lower=True,filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n',split=' ',char_level=False)

tokenizer.fit_on_texts(X)

x_train = tokenizer.texts_to_sequences(train)

x_test = tokenizer.texts_to_sequences(test)

word_index = tokenizer.word_index

X = tokenizer.texts_to_sequences(X)

vocab_size = len(word_index)
print('Vocab size: {}'.format(vocab_size))
longest = max(len(seq) for seq in X)
print("Longest comment size: {}".format(longest))
average = np.mean([len(seq) for seq in X])
print("Average comment size: {}".format(average))
stdev = np.std([len(seq) for seq in X])
print("Stdev of comment size: {}".format(stdev))
max_len = int(average + stdev * 3)
print('Max comment size: {}'.format(max_len))

from keras.preprocessing.sequence import pad_sequences

processed_x_train = pad_sequences(x_train, maxlen=max_len)
processed_x_test = pad_sequences(x_test, maxlen=max_len)

processed_pre_x_train = pad_sequences(x_train, maxlen=max_len)
processed_pre_x_test = pad_sequences(x_test, maxlen=max_len)

print('x_train shape:', processed_x_train.shape)
print('x_test shape:', processed_x_test.shape)

from tensorflow.keras.layers import Layer

import keras.backend
from keras.models import Sequential, load_model
from tensorflow.keras.layers import LSTM, Dense, Conv1D, MaxPooling1D
from tensorflow.keras.layers import Dropout, GlobalMaxPooling1D, BatchNormalization

from tensorflow.keras.models import Sequential, load_model

from keras.layers import Bidirectional
from tensorflow.keras.layers import Embedding

from keras.optimizers import Nadam
from tensorflow.keras.utils import plot_model

import matplotlib.pyplot as plt

import os

embeddings_index = {}
f = open(os.path.join('../input/glove-global-vectors-for-word-representation', '/content/drive/MyDrive/yelp_review_polarity_csv/yelp_review_polarity_csv/glove.6B.100d.txt'))
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()
print('Found %s word vectors.' % len(embeddings_index))

embedding_dim = 100
k = 0
embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        # Words not found in embedding index will be all-zeros.
        k += 1
        embedding_matrix[i] = embedding_vector

class Attention(Layer):
    def __init__(self, step_dim,
                 W_regularizer=None, b_regularizer=None,
                 W_constraint=None, b_constraint=None,
                 bias=True, **kwargs):

        self.supports_masking = True
        #self.init = initializations.get('glorot_uniform')
        self.init = initializers.get('glorot_uniform')

        self.W_regularizer = regularizers.get(W_regularizer)
        self.b_regularizer = regularizers.get(b_regularizer)

        self.W_constraint = constraints.get(W_constraint)
        self.b_constraint = constraints.get(b_constraint)

        self.bias = bias
        self.step_dim = step_dim
        self.features_dim = 0
        super(Attention, self).__init__(**kwargs)

    def build(self, input_shape):
        assert len(input_shape) == 3

        self.W = self.add_weight((input_shape[-1],),
                                 initializer=self.init,
                                 name='{}_W'.format(self.name),
                                 regularizer=self.W_regularizer,
                                 constraint=self.W_constraint)
        self.features_dim = input_shape[-1]

        if self.bias:
            self.b = self.add_weight((input_shape[1],),
                                     initializer='zero',
                                     name='{}_b'.format(self.name),
                                     regularizer=self.b_regularizer,
                                     constraint=self.b_constraint)
        else:
            self.b = None

        self.built = True

    def compute_mask(self, input, input_mask=None):
        # do not pass the mask to the next layers
        return None

    def call(self, x, mask=None):
        # eij = K.dot(x, self.W) TF backend doesn't support it

        # features_dim = self.W.shape[0]
        # step_dim = x._keras_shape[1]

        features_dim = self.features_dim
        step_dim = self.step_dim

        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))

        if self.bias:
            eij += self.b

        eij = K.tanh(eij)

        a = K.exp(eij)

        # apply mask after the exp. will be re-normalized next
        if mask is not None:
            # Cast the mask to floatX to avoid float64 upcasting in theano
            a *= K.cast(mask, K.floatx())

        # in some cases especially in the early stages of training the sum may be almost zero
        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())

        a = K.expand_dims(a)
        weighted_input = x * a
    #print weigthted_input.shape
        return K.sum(weighted_input, axis=1)

    def compute_output_shape(self, input_shape):
        #return input_shape[0], input_shape[-1]
        return input_shape[0],  self.features_dim

from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, concatenate
from tensorflow.keras.layers import Concatenate
from keras.models import Model

import keras.backend
from keras.models import Sequential, load_model
from tensorflow.keras.layers import LSTM, Dense, Conv1D, MaxPooling1D
from tensorflow.keras.layers import Dropout, GlobalMaxPooling1D, BatchNormalization
from keras.layers import Bidirectional
from tensorflow.keras.layers import Embedding
from keras.optimizers import Nadam
from tensorflow.keras.utils import plot_model
import matplotlib.pyplot as plt

from keras import backend as K

from keras.layers import Input, Embedding, Bidirectional, LSTM, Dropout, Attention, concatenate, Dense, BatchNormalization, Conv1D, GlobalMaxPooling1D
from keras.models import Model

def get_new_model():
    review_input = Input(shape=(max_len,), dtype='int32')
    review_input_post = Input(shape=(max_len,), dtype='int32')

    # First input branch
    x1 = Embedding(vocab_size + 1, embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=True)(review_input)
    x1 = Bidirectional(LSTM(60, return_sequences=True))(x1)
    x1 = Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')(x1)
    x1 = Dropout(0.5)(x1)
    x1 = BatchNormalization()(x1)
    attention_x1 = Attention()([x1, x1])

    # Second input branch
    x2 = Embedding(vocab_size + 1, embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=True)(review_input_post)
    x2 = Bidirectional(LSTM(60, return_sequences=True))(x2)
    x2 = Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')(x2)
    x2 = Dropout(0.5)(x2)
    x2 = BatchNormalization()(x2)
    attention_x2 = Attention()([x2, x2])

    # Concatenate both branches
    x = concatenate([attention_x1, attention_x2])

    # reduce dimensionality
    x = GlobalMaxPooling1D()(x)

    # Dense layer for classification
    x = Dense(60, activation='relu')(x)
    x = Dropout(0.4)(x)
    x = BatchNormalization()(x)

    preds = Dense(1, activation='sigmoid')(x)
    model = Model(inputs=[review_input, review_input_post], outputs=preds)
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

    return model

from keras.callbacks import ModelCheckpoint

from keras.callbacks import EarlyStopping
early_stopping_monitor = EarlyStopping(patience=25)

model1 = get_new_model()
model1.summary()

history = model1.fit([processed_x_train,processed_pre_x_train],y_train, validation_data=([processed_x_test ,processed_pre_x_test],y_test), epochs=15,batch_size=32,callbacks=[early_stopping_monitor],verbose=1)

# prompt:  generate a code to take iput and then predict if it is truthful or deceptive BASED ON THE ABOVE MODEL

from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Embedding, LSTM, Input, Dense, Dropout, Bidirectional, Conv1D, GlobalMaxPooling1D, BatchNormalization, concatenate
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Layer
import keras.backend as K
import numpy as np

# Assuming 'model1', 'tokenizer', 'max_len', 'word_index' are defined from the previous code

def predict_truthfulness(text):
    # Clean the input text
    text = clean_data(text)

    # Tokenize and pad the input text
    text_sequence = tokenizer.texts_to_sequences([text])
    padded_text = pad_sequences(text_sequence, maxlen=max_len, padding='post', truncating='post')

    # Predict the truthfulness
    prediction = model1.predict([padded_text, padded_text])  # Assuming your model takes two identical inputs

    if prediction[0][0] > 0.5:
        return "Truthful"
    else:
        return "Deceptive"

# Example usage:
input_text = input("Enter the text: ")
predicted_class = predict_truthfulness(input_text)
print("Predicted Class:", predicted_class)

model1.evaluate([processed_x_test,processed_pre_x_test],y_test)

from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score

y_pred = model1.predict([processed_x_test,processed_pre_x_test])
y_pred = (y_pred > 0.5)
accuracy = accuracy_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
print("Accuracy: {:.2f}".format(accuracy))
print("Recall: {:.2f}".format(recall))
print("Precision: {:.2f}".format(precision))
print("F1-score: {:.2f}".format(f1))

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

y_pred_prob = model1.predict([processed_x_test, processed_pre_x_test])

# Compute ROC curve and AUC
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)
roc_auc = auc(fpr, tpr)

# Plot ROC curve
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

from keras.layers import Input, Embedding, Bidirectional, LSTM, Dropout, Attention, concatenate, Dense, BatchNormalization
from keras.models import Model

def new_model():
    review_input = Input(shape=(max_len,), dtype='int32')
    review_input_post = Input(shape=(max_len,), dtype='int32')

    x1 = Embedding(vocab_size + 1, embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=True)(review_input)
    x1 = Bidirectional(LSTM(60, return_sequences=True))(x1)
    x1 = Dropout(0.5)(x1)
    x1 = BatchNormalization()(x1)
    attention_x1 = Attention()([x1, x1])

    x2 = Embedding(vocab_size + 1, embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=True)(review_input_post)
    x2 = Bidirectional(LSTM(60, return_sequences=True))(x2)
    x2 = Dropout(0.5)(x2)
    x2 = BatchNormalization()(x2)
    attention_x2 = Attention()([x2, x2])

    x = concatenate([attention_x1, attention_x2])
    x = GlobalMaxPooling1D()(x)
    x = Dense(60, activation='relu')(x)
    x = Dropout(0.4)(x)
    x = BatchNormalization()(x)
    preds = Dense(1, activation='sigmoid')(x)
    model = Model(inputs=[review_input, review_input_post], outputs=preds)

    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

model2 = new_model()
model2.summary()

history = model2.fit([processed_x_train,processed_pre_x_train],y_train, validation_data=([processed_x_test,processed_pre_x_test],y_test), epochs=15,batch_size=32,callbacks=[early_stopping_monitor],verbose=1)

model2.evaluate([processed_x_test,processed_pre_x_test],y_test)

# prompt:  generate a code to take iput and then predict if it is truthful or deceptive BASED ON THE ABOVE MODEL

from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Embedding, LSTM, Input, Dense, Dropout, Bidirectional, Conv1D, GlobalMaxPooling1D, BatchNormalization, concatenate
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Layer
import keras.backend as K
import numpy as np

# Assuming 'model1', 'tokenizer', 'max_len', 'word_index' are defined from the previous code

def predict_truthfulness(text):
    # Clean the input text
    text = clean_data(text)

    # Tokenize and pad the input text
    text_sequence = tokenizer.texts_to_sequences([text])
    padded_text = pad_sequences(text_sequence, maxlen=max_len, padding='post', truncating='post')

    # Predict the truthfulness
    prediction = model2.predict([padded_text, padded_text])  # Assuming your model takes two identical inputs

    if prediction[0][0] > 0.5:
        return "Truthful"
    else:
        return "Deceptive"

# Example usage:
input_text = input("Enter the text: ")
predicted_class = predict_truthfulness(input_text)
print("Predicted Class:", predicted_class)

y_pred = model2.predict([processed_x_test,processed_pre_x_test])
y_pred = (y_pred > 0.5)
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score
accuracy = accuracy_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
print("Accuracy: {:.2f}".format(accuracy))
print("Recall: {:.2f}".format(recall))
print("Precision: {:.2f}".format(precision))
print("F1-score: {:.2f}".format(f1))

y_pred_prob = model2.predict([processed_x_test, processed_pre_x_test])

fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)
roc_auc = auc(fpr, tpr)

# Plot ROC curve
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Generate predicted probabilities for both models
y_pred_prob_model_1 = model1.predict([processed_x_test, processed_pre_x_test])
y_pred_prob_model_2 = model2.predict([processed_x_test, processed_pre_x_test])

# Compute ROC curve and AUC for both models
fpr_model_1, tpr_model_1, _ = roc_curve(y_test, y_pred_prob_model_1)
roc_auc_model_1 = auc(fpr_model_1, tpr_model_1)

fpr_model_2, tpr_model_2, _ = roc_curve(y_test, y_pred_prob_model_2)
roc_auc_model_2 = auc(fpr_model_2, tpr_model_2)

# Plot both ROC curves on the same figure
plt.figure()

# ROC curve for Model 1
plt.plot(fpr_model_1, tpr_model_1, color='darkorange', lw=2,
         label='Model 1 ROC curve (area = %0.2f)' % roc_auc_model_1)

# ROC curve for Model 2
plt.plot(fpr_model_2, tpr_model_2, color='blue', lw=2,
         label='Model 2 ROC curve (area = %0.2f)' % roc_auc_model_2)

# Diagonal line representing random performance
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')

plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic Comparison')
plt.legend(loc="lower right")
plt.show()

# Plot training & validation accuracy values
plt.plot(history.history['accuracy'])
# plt.plot(history.history['val_acc'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

# Plot training & validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()