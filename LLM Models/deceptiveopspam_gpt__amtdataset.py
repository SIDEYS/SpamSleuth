# -*- coding: utf-8 -*-
"""DeceptiveOpSpam_GPT__AMTDataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HF08z9LMsGzK0dJZEVzd1XVJZzks3Wuk

# To Bypass Proxy Configurations

# Check if GPU is available and set device accordingly
"""

from google.colab import drive
drive.mount('/content/drive')

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from transformers import GPT2Tokenizer, GPT2Model
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# Load CSV
df = pd.read_csv("/content/fake_review_old.csv")

# Check columns
assert 'text' in df.columns and 'label' in df.columns, "Columns 'text' and 'label' must exist in CSV."

# Encode labels
label_encoder = LabelEncoder()
df['label'] = label_encoder.fit_transform(df['label'])

# Split into train/test
train_data, test_data = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])

# Check CUDA
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Load Tokenizer
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token  # GPT2 has no pad token, so use eos_token

# Custom Dataset
class TextDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        encoding = self.tokenizer(
            self.texts[idx],
            padding='max_length',
            truncation=True,
            max_length=self.max_length,
            return_tensors='pt'
        )
        return {
            "input_ids": encoding['input_ids'].squeeze(),
            "attention_mask": encoding['attention_mask'].squeeze(),
            "labels": torch.tensor(self.labels[idx], dtype=torch.long)
        }

# Create Datasets and DataLoaders
train_dataset = TextDataset(train_data['text'].tolist(), train_data['label'].tolist(), tokenizer)
test_dataset = TextDataset(test_data['text'].tolist(), test_data['label'].tolist(), tokenizer)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=16)

# Define Model
class GPTClassifier(nn.Module):
    def __init__(self, num_classes):
        super(GPTClassifier, self).__init__()
        self.gpt = GPT2Model.from_pretrained("gpt2")
        self.fc = nn.Linear(self.gpt.config.hidden_size, num_classes)

    def forward(self, input_ids, attention_mask):
        outputs = self.gpt(input_ids=input_ids, attention_mask=attention_mask)
        logits = self.fc(outputs.last_hidden_state[:, -1, :])
        return logits

num_classes = len(label_encoder.classes_)
model = GPTClassifier(num_classes).to(device)

# Training Setup
criterion = nn.CrossEntropyLoss()
optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.04)

def train_model(model, train_loader, test_loader, epochs=5):
    for epoch in range(epochs):
        model.train()
        total_loss, total_correct, total_samples = 0, 0, 0

        for batch in train_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            optimizer.zero_grad()
            logits = model(input_ids, attention_mask)
            loss = criterion(logits, labels)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            preds = torch.argmax(logits, dim=1)
            total_correct += (preds == labels).sum().item()
            total_samples += labels.size(0)

        train_loss = total_loss / len(train_loader)
        train_acc = (total_correct / total_samples) * 100

        # Evaluation
        model.eval()
        test_loss, test_correct, test_samples = 0, 0, 0
        all_preds, all_labels = [], []

        with torch.no_grad():
            for batch in test_loader:
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels = batch['labels'].to(device)

                logits = model(input_ids, attention_mask)
                loss = criterion(logits, labels)

                test_loss += loss.item()
                preds = torch.argmax(logits, dim=1)
                test_correct += (preds == labels).sum().item()
                test_samples += labels.size(0)

                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())

        test_loss /= len(test_loader)
        test_acc = (test_correct / test_samples) * 100

        print(f"\nEpoch [{epoch+1}/{epochs}]")
        print(f"Train Loss: {train_loss:.4f}, Accuracy: {train_acc:.2f}%")
        print(f"Test  Loss: {test_loss:.4f}, Accuracy: {test_acc:.2f}%")

        # Classification Report
        print("Classification Report:")
        print(classification_report(all_labels, all_preds))

        # Confusion Matrix
        cm = confusion_matrix(all_labels, all_preds)
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
        plt.xlabel("Predicted")
        plt.ylabel("Actual")
        plt.title("Confusion Matrix")
        plt.show()

        # ROC Curve
        if num_classes <= 5:  # Avoid clutter for many classes
            all_labels_bin = np.eye(num_classes)[all_labels]
            all_preds_bin = np.eye(num_classes)[all_preds]
            plt.figure(figsize=(8, 6))
            for i in range(num_classes):
                fpr, tpr, _ = roc_curve(all_labels_bin[:, i], all_preds_bin[:, i])
                plt.plot(fpr, tpr, label=f"{label_encoder.classes_[i]} (AUC = {auc(fpr, tpr):.2f})")
            plt.plot([0, 1], [0, 1], 'k--')
            plt.xlabel("False Positive Rate")
            plt.ylabel("True Positive Rate")
            plt.title("ROC Curve")
            plt.legend()
            plt.show()

# Train Model
train_model(model, train_loader, test_loader, epochs=7)

import torch.nn.functional as F
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt
import numpy as np

def plot_roc_curve(model, test_loader, num_classes, label_encoder):
    model.eval()
    all_labels = []
    all_probs = []

    with torch.no_grad():
        for batch in test_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            # Forward pass
            outputs = model(input_ids, attention_mask)

            # Convert logits to probabilities
            probs = F.softmax(outputs, dim=1)

            all_labels.extend(labels.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())

    all_labels = np.array(all_labels)
    all_probs = np.array(all_probs)

    # Compute ROC curve and AUC for each class
    plt.figure(figsize=(8, 6))

    for i in range(num_classes):
        binary_labels = (all_labels == i).astype(int)  # Convert to binary for each class
        fpr, tpr, _ = roc_curve(binary_labels, all_probs[:, i])  # Get ROC curve
        roc_auc = auc(fpr, tpr)  # Compute AUC

        plt.plot(fpr, tpr, label=f"{label_encoder.classes_[i]} (AUC = {roc_auc:.2f})")

    # Plot settings
    plt.plot([0, 1], [0, 1], "--", color="gray")  # Diagonal baseline
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("ROC Curve for Each Class")
    plt.legend()
    plt.grid()
    #plt.show()
    plt.savefig("ROC_Gpt.pdf", format = 'pdf', dpi =1000, bbox_inches = 'tight')

# Call function to plot ROC curve
plot_roc_curve(model, test_loader, num_classes, label_encoder)

import torch
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

def plot_confusion_matrix(model, test_loader, label_encoder):
    model.eval()
    all_labels = []
    all_preds = []

    with torch.no_grad():
        for batch in test_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            # Forward pass
            outputs = model(input_ids, attention_mask)
            preds = torch.argmax(outputs, dim=1)  # Get predicted class indices

            all_labels.extend(labels.cpu().numpy())
            all_preds.extend(preds.cpu().numpy())

    # Compute confusion matrix
    cm = confusion_matrix(all_labels, all_preds)
    class_names = label_encoder.classes_  # Get class labels

    # Plot confusion matrix
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=class_names, yticklabels=class_names)
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.title("Confusion Matrix")
    #plt.show()
    plt.savefig("Confusion_gpt_25.pdf", format = 'pdf', dpi =1000, bbox_inches = 'tight')

    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    plt.figure(figsize=(8, 6))
    sns.heatmap(cm_normalized, annot=True, fmt=".2f", cmap="Blues", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.title("Normalized Confusion Matrix")
    #plt.show()
    plt.savefig("Confusion_gpt_normalized_25.pdf", format = 'pdf', dpi =1000, bbox_inches = 'tight')

# Call function to plot confusion matrix
plot_confusion_matrix(model, test_loader, label_encoder)